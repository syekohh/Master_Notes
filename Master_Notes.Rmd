---
title: "\\textbf{\\Huge STA258 Master Notes}"
author: "\\LARGE Haris Aljic"
date: "\\large `r Sys.Date()`"
output:
  pdf_document: 
    toc: true
    toc_depth: 3
  html_document:
    toc: true
header-includes:
   - \usepackage{setspace}
   - \onehalfspacing
   - \usepackage{titling}
   - \predate{\begin{center}\LARGE}
   - \postdate{\end{center}\vspace{3.5cm}\begin{center}\includegraphics[width=3.9in,height=3.9in]{/Users/harisaljic/Downloads/p-value-statistics-meme.jpg}\vspace{3.5cm}\end{center}}
   - \preauthor{\vspace{0.1cm}\begin{center}\LARGE}
   - \postauthor{\end{center}}
   # - \usepackage[none]
   - \usepackage{wrapfig}
   - \usepackage{lipsum}
   - \usepackage{color}
   - \usepackage{framed}
   - \setlength{\fboxsep}{.8em}
   - \usepackage{tcolorbox}
   - \usepackage{xcolor}
   - \tcbuselibrary{listings,breakable}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\definecolor{myorange}{RGB}{255,165,0}
\definecolor{mygreen}{RGB}{0,204,0}
\definecolor{myblue}{RGB}{3,165,252}
\definecolor{mylightblue}{RGB}{3,206,252}

\newtcolorbox{mydefinitionbox}{
    colback=myorange!10, 
    colframe=myorange,   
    coltext=black,      
    fonttitle=\bfseries, 
    title=Definition,    
    breakable,           
}

\newpage

# Module 1 - "Exploring Categorical Data"

* Categorical variables represent groups/categories like colour/gender/etc
* They can be nominal (non-ordered) or ordinal (ordered)
  - This is a "scale of measure" for the data
* They're graphical displays are most often bar graphs and pie charts

## Frequency Distributions

* A "distribution" is a variables _pattern of variation_
* A frequency distribution orders a set of scores from highest to lowest
* Can be modeled as either a table or graph, but regardless, the _same two elements_ are presented
  - Set of categories that make up the original measurement scale
  - A record of the frequency, or number of individuals in each category
* So a frequency distribution presents a **picture** of how individual scores/observations are distributed on the measurement scale

## Describing Categorical Data

* Frequency (counts)
* Relative frequency (proportion)
  - Count of category divided by total count
* Percentage (proportion times 100)

This information is often _derived from_ or _given_ in a Frequency Distribution Table (and graphically displayed as either a bar graph or pie chart)

**Consider the Titanic Example**

```{r}
titanic <- read.csv("Titanic.csv")
attach(titanic)
str(titanic)
```

We can see two types of distribution tables here for the "Survived" variable:

```{r}
# See the Frequency Distribution Table for the Survived variable
addmargins(table(Survived))
```

This table tells us precisely HOW MANY (frequency/count) people survived or not

\newpage

``` {r}
# See the RELATIVE Frequency Distribution Table for the Survived variable
addmargins(prop.table(table(Survived)))
```

This table tells us the relative frequency (proportion) of survivors to the total population

We can extend this further to say that $\approx 32\%$ survived, for example.

## Variable Roles

A variable can be classified as **Response (outcome, dependent)**, or **Explanatory (predictor, independent)**

A study on two variable where one is a **response variable**, the other **explanatory**, we observe the outcomes of the response _depends_ on the values of the explanatory

## Contingency Tables and Marginal Distribution Tables

**Consider the Ticket Classes for the Titanic dataset**

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(mosaic)
```
```{r, echo=FALSE}
tally(~ Class, margin = TRUE, data = titanic)
```

And define the contingency table as follows:
```{r}
Contingency.Table <- table(Survived, Class)
addmargins(Contingency.Table)
```
(Note this table is still classified as 2x4)

Notice how for any single cell, three different percentages exist.

For example, take the number of First Class passengers that are alive ($203$):

* Row percentage: $203/711 = 28.6\%$ of survivors in first-class
* Column percentage: $203/325 = 62.5\%$ of first-class passengers survived
* Overall percentage: $203/2201 = 9.2\%$ of passengers were in first-class

## Marginal Distributions and Joint Distributions

The sum for each row or for each column give totals. These are the _margins_. In the Ticket Class example, we have two variables - **Survived**  (row) and **Ticket Class** (column)

Marginal distributions can be displayed as counts or percentages.

```{r, echo=FALSE}
addmargins(prop.table(margin.table(Contingency.Table, 1)))
```

Similar to Marginal Distributions are Joint Distributions, where each cell of the dataset sums together to the overall total, like so:

```{r, echo=FALSE}
addmargins((prop.table(Contingency.Table)))
```

## Comparing Two Conditional Proportions (Difference and Ratio of Proportions)

The **Difference of Proportions** is simply subtracting one proportion from another to determine which has higher **percentage points**

For example, with the ticket class data set, the first-class passengers who survived ($62\%$), is $37$ **percentage points higher** than the third-class passengers ($25\%$)

It is incredibly important to note that the percentage of first-class survivors is NOT $37\%$ higher than the percentage of third-class survivors. It is $37$ **percentage points** higher.

The **Ratio of Proportions**, however, tells us much larger or smaller one proportion is to another.

Following the same example from earlier, 0.62/0.25 = 2.48 implies that the proportion of survived is 2.48 times larger for first-class passengers than for third-class (or that first-class passengers were 2.48 times more likely to have survived than third-class)

**Interpreting Ratios**

* When proportions are (nearly) identical, their difference is zero, ratio is one, and are said to have **no association** - The greater their difference, the stronger their association.
* Ratio of proportions is often preferred to the difference of proportions when the proportions themselves are both small.
* It is common to speak of *percent increases* when you have a ratio in between 1 and 2
  - For example, a ratio $\frac{a}{b} = 1.09$ implies that proportion $a$ is 1.09 times larger than $b$, and hence is 9% higher.
* Conversely, a ratio below 1 implies a percent decrease - $\frac{b}{a} = 0.91 \implies 100\% - 91\% = 9\%$ decrease (note this is just the reciprocal of the earlier fraction)


# Module 2 - "Exploring Quantitaive Data"

## Shapes of Histograms

![](/Users/harisaljic/STA258_R/Master_Notes/Plot_Examples.png)
An easy way to remember, is that the type of skewness corresponds to the tail. For example, if most of the observations are on the right side, that means there's a tail to the left, which means the distribution is **left-tailed** so it is also **left-skewed**.

```{r, echo=FALSE, fig.align='center', fig.height=3, fig.width=5}
# Set the seed for reproducibility
set.seed(292)

# Generate random numbers for the first mode
mode1 <- rnorm(1000, mean = 5, sd = 1.5)

# Generate random numbers for the second mode
mode2 <- rnorm(1000, mean = 15, sd = 1.5)

# Combine the two modes
bimodal_distribution <- c(mode1, mode2)

# Plot the bimodal distribution
hist(bimodal_distribution, col = "lightblue", main = "Bimodal Distribution (Two Modes)", xlab = "Value")
```

## Density Plot

A **Density Plot** is a smooth, continuous function over an interval to visualize the distribution of data. The advantage of density plots over histograms is that they are unaffected by the number of bins (bars) used, and can have a shape or form with lower bin counts that histograms typically would not. 

Another detail to consider when discussing density plots is their **bandwidth selection**. Higher bandwidth smooths out the curve, and will likely leaves out smaller variations. Lower bandwidth, however, shows the much finer details and fluctuations. Take a look below to see the difference bandwidth makes on the _same_ dataset:

```{r, echo=FALSE, fig.align='center', fig.height=3, fig.width=5}

tims <- read.csv("TimHortons.csv")
attach(tims)

# Set seed for reproducibility
set.seed(292)

# Plot the density curves
par(mfrow = c(1, 2))  # Set up a 1x2 grid for side-by-side plots

# Plot the first density curve (higher bandwidth)
plot(density(Calories, bw = 50), main = "Higher Bandwidth", col = "blue", lwd = 2)

# Plot the second density curve (lower bandwidth)
plot(density(Calories, bw = 5), main = "Lower Bandwidth", col = "red", lwd = 2)

# Reset the plotting parameters
par(mfrow = c(1, 1))
```

## Centre and Central Tendency

The **centre** is defined to be a value in the data, whereas the **Central Tendency** is a statistical measure (like the mean, median, or mode) used to determine a single score that _defines_ the centre of a distribution.

Our goal surrounidng the central tendency is to find a single score that best represents the data of the entire group.

## Mean, Median, and Mode

### Mean

The mean can be described as a balance point, located in between the lowest and highest points of data. The total distance below the mean is equal to to total distance aboove the mean.

Note: The mean is sensitive to extremely large and small cases.

Formula: Sum all observations from the variable and divide by the total count

We discuss two different types of means - the **population mean** and the **sample mean**

The **population mean** (denoted by $\mu$), is found by adding every single value in an entire population, and dividing it by the total population size

The **sample mean** (denoted by $\bar{x}$), however, is found by adding only a selection of the values from the population, and dividing by the size of said collection. This selection is called a *sample*.



### Median

The median is the middle value of the data, when sorted from smallest to largest. It is the midpoint of the list. Median can also be denoted the **50th percentile** - a point on a scale such that 50% of the values are below it, and 50% of the values are above it.

Note: The median is resistant to extremely large and extremely small data points. This is because the median is not found by taking literal data points into account, unlike the mean - **We will report the median for skewed distribution**

Formula:

* Case 1: Odd number of data
  - Order the data from smallest to largest
  - The data point in the $\frac{n + 1}{2}$ position (middle) is the median.
* Case 2: Even number of data
  - Order the data from smallest to largest
  - Calculate the average (mean) of the values in the $\frac{n}{2}$ and $\frac{n}{2} + 1$ positions (i.e. $\frac{entry\ at(\frac{n}{2}) + entry\ at(\frac{n}{2} + 1)}{2}$) to find the median.

**Note:** Do NOT rely on mean and median values to determine the shape of the data. It can be that the mean and median are approximately or nearly equal but data is still skewed.

Consider the Tim Hortons data.

```{r}
# Find the Mean, then Median
mean(Calories)

median(Calories)
```

```{r, echo=FALSE, fig.align='center', fig.height=2.75, fig.width=3.75}
hist(Calories, col = "bisque2")
```

The histogram is right skewed, despite the mean (251) and median (250) are very close in value.

Interpretation of the mean and median for calorie counts in donuts:

* Mean: On average, the donuts have 251 calories.
* Median: Half of the donuts have at least 250 calories, where as the other half have at most 250 calories

### Mode

The mode of a variable refers to the most frequently appearing data point. In a frequency distribution graph, the modes are the peaks of the graph (plural because there may be multiple modes - two or more different values could each share the same largest frequency)

Items of note:

* Symmetrical Distributions
  - The mean and median are equal and exactly at the centre
  - If the distribution is only approximately symmetric, the mean and median are clsoe to each other
  - Unimodal distributions (one mode) with mean = median = mode implies the distribution is bell-shaped symmetric
  - Bimodal distributions (two modes) can still be symmetric
* Skewed Distributions
  - Occurs when there exists an observation(s) that deviate from the general pattern of data.
  - This effects where you can find the mean, median, and mode.
    - Left/Negative skewed: Mean first, then median, then mode
    - Right/Positive skewed: Mode first, then median, then mean
  - In other words:
    - Mean < Median $\implies$ left skewed
    - Median < Mean $\implies$ right skewed
  - In general, just looking at the values is not always good enough to determine the shape of the data (for example, the distribution could just be bimodal)


## Spread - Sample Variance (and Sample Standard Deviation)

The spread of a set of data tells us how much data varies around its centre. In other words, how far from the mean/median do observations tend to be?

The **Variability** describes the distribution. It is a quantitative measure of the differences between observations, describing **how** spread out or **how** grouped together these data points are. It measures how well either a single score, or group of scores represents the entire distribution, as well as how much _error_ to expect when using a sample to represent a population.

Describing a data set numerically typically requires a report of its spread, and the centre.

The variance will help us (indirectly) determine the spread of the data in a sample.

### Sample Variance

Sample variance measures the spread about the sample mean, $\bar{x}$

Formula: $S^2 = \frac{\Sigma_{i=1}^n(x_i - \bar{x})^2}{n - 1}$

Taking the square root leaves us with **sample standard deviation**, which is the value that will tell us roughly on average how much values differ from the mean.

Here:

  - $x_i$ = $i^{th}$ observation in the data
  - $\bar{x}$ = sample mean
  - $n$ = sample size

Note: The sample variance has (n - 1) degrees of freedom since one value out of the sample is dependent on all the others to determine the sample variance - (n - 1) values are "free".

### Population Variance

Formula: $\sigma^2 = \frac{\Sigma_{i=1}^N(x_i - \mu)^2}{N}$

Taking the square root leaves us with **population standard deviation**, which is the value that will tell us roughly on average how much values differ from the mean.

Here:

  - $x_i$ = $i^{th}$ observation in the data
  - $\mu$ = Population mean
  - $n$ = Population size

Note: Unlike sample variance, population variance divides by the total population size, $N$.

It it important to see that the standard deviation is always greater than or equal to zero. ($\sigma, S \geq 0$), and the _larger_ the standard deviation, the _greater_ the variability.

(Standard deviation also rescales when the data is rescaled)

## Percentiles, Quartiles, and Interquartile Range

Distributions can also be described with a measure of position. For example, **range = max data point - min data point**. However, some measures describe **centre**, and some **variability**.

### Percentiles

The $p^{th}$ percentile is a point in a data set where $p\%$ of the observations fall under it (or equivalently, $(100 - p)\%$ fall above it).

For example, consider the $50^{th}$ percentile. This is equivalent to the _median_, since half the values are above it, and the other half below.

We should familiarize ourselves with some common terminology surrounding percentiles before proceeding further.

**Quartile** corresponds to a quarter of the data, so it is only used when discussing 0, 25, 50, 75, and 100 percent of the data.

**Quantile** is a decimal value between zero and one used as a decimal representation for the percentage $p$

**Percentile** is how we defined our $p$, to be the point where $p\%$ of the observations fall under it

For example, let p = 25, then:

  - "0.25 quantile" = "$25^{th}$ percentile" = "First quartile"/"1 quartile/Q1"

Now let p = 33, then:

  - "0.33 quantile" = "$33^{rd}$ percentile" = "quantile not defined for this value"

For the remainder of these notes, I will try to primarily refer to first, second, and third quartiles as Q1, Q2, and Q3 respectively.

Special Definition: (Sample Percentile)
* The $i^{th}$ smallest observation in an ordered list is called the **sample $[\frac{100(i - 0.5)}{n}]$th percentile**

### Interquartile Range (IQR)

The IQR is defined to be the middle half of the data, where 50% of the data falls.

Since Q1 contains the first 25% of the data below it, and Q3 contains the last 25% above it, the IQR is simply Q3 - Q1. In other words, the $75^{th}$ percentile - $25^{th}$ percentile. The IQR is less affected by outliers (sometimes it's not affected at all).

See the below illustration for how the IQR works with boxplots

![](/Users/harisaljic/STA258_R/Master_Notes/Boxplot_Guide.png)

For boxplots, we determine shape based on the position of the median (centre line in the box). 

* If the line is further to the **right/top**, it is **left skewed**.
* Conversely, if the line is further to the **left/bottom**, it is **right skewed**. 
  - You can think of it as "the tyoe of skewness corresponds to the side with more space".
  - In the above illustration, the data would have a _right skew_.
* Otherwise, a median line in the middle of the box implies the data is approximately symmetric.

Centre is reported as the position of the median line in the box.

Spread is reported in terms of the range of the box - ie, Q3 - Q1.

## Z-Scores

A **Z-Score** refers to a **standardized data value**, representing how far an observation is away from the mean **in terms of standard deviations**

$Z = \frac{observation - mean}{standard\ deviation} = \frac{x - \mu}{\sigma}$

Note this example uses population mean and standard deviation. For a sample, simply swap $\mu$ for $\bar{x}$ and $\sigma$ for $S$.

* If your Z-score is positive, the observation lies **above** the mean
* If your Z-score is negative, the observation lies **below** the mean
* A data point is more unusual in the collection the greater the Z-score is in magnitude
  - it is very rare for a bell-shaped symmetric distribution to have values falling more than 3 standard deviations above or below the mean

Standardizing into Z-scores shifts the data to have the mean centred at zero, and rescales for the standard deviation to become 1. This does not change the shape of the distribution.

### Empirical Rule

The empirical rule states that for distributions that are approximately bell-shaped symmetric, without outliers, certain fixed (approximate) percentages of the sample fall within certain ranges of the graph.

* $\approx 68\%$ of the data falls within one SD (standard deviation) of the mean - $\mu \pm \sigma$
* $\approx 95\%$ of the data falls within two SD's of the mean - $\mu \pm 2\sigma$
* $\approx 99.7\%$ of the data falls within three SD's of the mean - $\mu \pm 3\sigma$

## Different Plots and how they relate to Normal Distributions

You can tell if a boxplot is bell-shaped symmetric if $\frac{IQR}{S} \approx 1.33$

Below are some probability plots for different shapes of distributions:

```{r, echo=FALSE, fig.align='center', out.height="50%", out.width="50%"}
library(knitr)
include_graphics("/Users/harisaljic/STA258_R/Master_Notes/Plot_Types.png")
```


# Module 3 - "Sampling Distributions Related to a Normal Population"

**Facts about the Normal Distribution** ($X \overset{\mathrm{iid}}{\sim} N(\mu, \sigma)$)

* They are bell-shaped symmetric, smooth curves (this is not unique to normal distributions)
* The mean is located at the centre of the curve
* The mean, median, and mode are equal
* The standard deviation control the spread of the curve
* It models most real-world distributions

**From a more mathematical perspective**

* The maximum is located at $x = \mu$
* Inflection points occur at $x = \mu \pm \sigma$
* It is symmetric about $x = \mu$
* The $x$-axis is a horizontal asymptote

## Moments

Let $Y$ be a random variable.

The Expected Value ($E[Y]$) is called the "first moment" of $Y$, $E[Y^2]$ is the second moment, and so on.

Generalized, $E[Y^k]$ is the $k^{th}$ moment of $Y$.

Moreover, $E[(Y - E[Y])] = 0$ is defined as the "first **central** moment" of $Y$, and $E[(Y - E[Y])^2]$ as the second central moment.

Take note that that second central moment is equivalent to the definition of the variance for the random variable $Y$: $Var(Y) = E[(Y - E[Y])^2]$

## Moment Generating Functions

The moment generating function is defined as $M_X(t) = E[e^{tX}]$, a function of $t$, and not any random variables. This function must be differentiable at $t = 0$. $M_X(t)$ is most often expressed as a closed form.

These are useful for finding expectation and variance of random variables, as the $k^{th}$ derivative evaluated at $t = 0$ produces the $k^{th}$ moment.

So now we have that:

* $E[X] = M'_X(t = 0)$
* $Var(X) = E[X^2] - (E[X])^2 = M''_X(t = 0) - (M'_X(t = 0))^2$

## The Sample Mean being related to the Normal Distribution

The Sample Mean is defined as follows:

$\bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i$

Moreover, if each $X_i$ comes from a normal distribution with mean $\mu$, standard deviation $\sigma$, we can define the sampling distribution of the sample mean $\bar{X}$ as well:

If $X_1, X_2, \dots , X_n \overset{\mathrm{iid}}{\sim} N(\mu, \sigma)$, then $\bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i$ ~ $N(\mu, \frac{\sigma}{\sqrt{n}})$

Here, $\frac{\sigma}{\sqrt{n}}$ is called the **standard error** of the sample mean, and it is denoted by $\sigma_{\bar{X}}$. It is a fraction of the spread of the data.

# Module 4 - "Sampling Distributions Related to a Normal Population (cont.'d)"

## The Chi-Square Distribution ($\chi_{\nu}^2$)

Let $Z_1, Z_2, \dots, Z_n \overset{\mathrm{iid}}{\sim} N(0, 1)$, and define a variable $U$ such that...

  $$U = Z_1^2 + Z_2^2 + \dots + Z_n^2$$

We say that $U$ follows a chi-squared distribution with $n$ degrees of freedom, denoted as $\chi_{(n)}^2$

Some general properties of the chi-squared distribution:

* The sum of independent chi-squared distributions is also chi-squared, with degrees of freedom equal to the sum of all degrees of freedom for those being summed.
  - $\chi_{\nu_1}^2$ + $\chi_{\nu_2}^2$ = $\chi_{\nu}^2$ where $\nu = \nu_1 + \nu_2$
* A $\chi_{\nu}^2$ also follows a **Gamma distribution**, with $\alpha = \frac{\nu}{2}$, $\beta = 2$
* The greater the degrees of freedom (df), the more spread out and approximately bell-shaped symmetrical the curve (distribution) gets.

### The Relation of $\chi^2$ on Sample Variance $S^2$

If $Z_1, Z_2, \dots, Z_n \overset{\mathrm{iid}}{\sim} N(0, 1)$, then let $U$ be a R.V where $U = \frac{n - 1}{\sigma^2}S^2 = \sum_{i = 1}^{n}(\frac{X_i - \bar{X}}{\sigma})^2 \sim \chi_{n - 1}^2$

That is, $U$ follows a chi-square distribution with $(n - 1)$ degress of freedom.

Also note that $\bar{X}$ and $S^2$ are **independent random variables**, where:

* $E[S^2] = \sigma^2$
* $Var(S^2) = \frac{2\sigma^4}{n - 1}$

Special Note: $U$ only has $(n - 1)$ degrees of freedom when we subtract by the sample mean, $\bar{X}$, but subtracting by **population mean** results in a chi-squared with exactly $n$ degrees of freedom since _all_ variables are random/"free", and none are fixed/"not free"

## The t-Distribution

Let $Z\ {\sim}\ N(0, 1)$, and $W\ \sim\ \chi_{\nu}^2$, then define a variable $T$ such that...

  $$T = \frac{Z}{\sqrt{\frac{W}{\nu}}}\ \sim\ t_{\nu}$$
Some general properties of the chi-squared distribution:

* Its support is defined to be the real numbers
* $E[T] = 0$
* $Var(T) = \frac{\nu}{\nu - 2} > 1$ where $Var(T) \rightarrow 1$ as $\nu \rightarrow \inf$
* $t_{\nu}$ is shorter and fatter than the standard normal distribution, and the tails approach zero much slower.
* As the degrees of freedom approaches infinity, the t-distribution converges to a standard normal
* $\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim t_{n - 1}$
* Typically used for sample sizes of **less than 30**

## The F-Distribution

A random variable $F$ is said to follow an $F$ distribution if...

$$F = \frac{\frac{\chi_{\nu_1}^2}{\nu_1}}{\frac{\chi_{\nu_2}^2}{\nu_2}} \sim F_{\nu_1,\ \nu_2}$$
For $\nu_2 > 2$

* The support of $F$ is defined on the non-negative real numbers.
* $E[X] =\frac{\nu_2}{\nu_2 - 1}$ for $\nu_2 > 2$, and $\infty$ for $0 < \nu_{2} \leq 2$
* $Var(X) = \infty$ for $2 < \nu_{2} \leq 4$, undefined for $0 < \nu_{2} \leq 2$, and messy for $\mu_2 > 4$

Some additional properties:

* For $F \sim F_{\nu_1,\ \nu_2}$, $\frac{1}{F} \sim F_{\nu_2, \nu_1}$ (the df switch)
* Let $T \sim t_{\nu}$. Then, $T^2 \sim F_{1,\ \nu}$
* If $E_1, E_2 \overset{\mathrm{iid}}{\sim}\ $ Exponential$(\beta)$, $\frac{E_1}{E_2} \sim F_{2, 2}$

Consider two samples of data:

* $X_1, X_2, \dots, X_n \overset{\mathrm{iid}}{\sim} N(\mu_X, \sigma)$
* $Y_1, Y_2, \dots, Y_m \overset{\mathrm{iid}}{\sim} N(\mu_Y, \sigma)$

This implies that $\frac{n - 1}{\sigma^2}S_X^2 \sim \chi_{n - 1}^2$, and $\frac{m - 1}{\sigma^2}S_Y^2 \sim \chi_{m - 1}^2$

So then,

$$\frac{(\frac{\frac{n - 1}{\sigma^2}S_X^2}{n - 1})}{(\frac{\frac{m - 1}{\sigma^2}S_Y^2}{m - 1})} = \frac{\frac{S_X^2}{\sigma^2}}{\frac{S_Y^2}{\sigma^2}} = \frac{S_X^2}{S_Y^2} \sim F_{n - 1, m - 1}$$

# Module 5 - "Applications of the Limit Theorem"

## Chebyshev's Theorem

Chebyshev's Theorem defines a bound an observation falling $k$-many standard deviations from the mean. The bounds are as follows:

$$P(|X - \mu| \leq k\sigma) > 1 - \frac{1}{k^2}$$
In simple terms, this states that the probability of some observation ($X$) being **within** $k$ standard deviations from the mean is **at least** $1 - \frac{1}{k^2}$

$$P(|X - \mu| > k\sigma) \leq \frac{1}{k^2}$$

In simple terms, this states that the probability of some observation ($X$) being $k$ standard deviations **away** from the mean is **at most** $\frac{1}{k^2}$

This is a rule that can apply to any type of distribution.

## Convergence in Probability and the Weak Law of Large Numbers

The Weak Law of Large Numbers (WLLN), in short, states that in a sample size $n$-many idependent and identically distributed random variables with common mean (expected value) $\mu$, the sample mean gets increasingly close to the true population mean as the size of the sample, $n$, increases.

A more symbolic and mathematical definition to the WLLN can be shown through either of the following limits:
$$\lim_{{n \to \infty}} P(|\bar{X}_n - \mu| \geq \epsilon) = 0\ /\  \lim_{{n \to \infty}} P(|\bar{X}_n - \mu| \leq \epsilon) = 1$$
Although a powerful rule of probability and statistics, the Weak Law of Large Numbers comes from a more general concept known as **convergence in probability**, which states that for a sequence of $n$-many random variables, as $n$ approaches infinity, the probability that the sequence differs from some random variable $X$ becomes arbitrarily small (zero).

## Central Limit Theorem (CLT)

The CLT states that the sample mean of a sample of **iid random variables** converges to a standard normal distribution ($N(0, 1)$)

## Normal Approximation to Binomial

The Binomial distribution is essentially a list of $n$-many Bernoulli (yes/no) observations.

Given $X \sim Binomial(n, p)$, this can be approximated to a normal distribution $Y$ where $Y \sim N(np, np(1-p))$.

Even further, we can standardize this using the Central Limit Theorem to be able to use the Z-table, as all the Bernoulli observations are independent and identically distributed.

# Module 6 - "Confidence Intervals"

Denote $\hat{\theta}$ as a point estimator of the parameter of some distribution, such as $\lambda$ for Poisson, $\beta$ for Exponential, the median, etc. $\hat{\theta}$ is a random variable in it of itself.

These point estimators, however, are not perfect, and are bound to have some error in their estimation. This bring sus to the idea of **confidence intervals (CI)**. Confidence Intervals provide a range of real values that our estimate falls in. The shorter the width (upper bound minus lower bound) of the interval, the better the decisions we can make are.

This can be expressed as:
$$P(\hat{\theta}_L < \theta < \hat{\theta}_U) = (1 - \alpha)$$
We need to have a pivotal quantity to find an estimator for the confidence interval.

Some common pivotal quantities:

- $U = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$ for a sample following $N(\mu, \sigma)$ with unknown mean and known variance.
- $U = \frac{(n - 1)S^2}{\sigma^2}$ for a sample following $N(\mu, \sigma)$ with known mean and unknown variance.

## CI for the Mean of a Normal Population (Variance Known)

For a 95\% confidence interval...

Since $U = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$ is a pivotal quantity here, and we know $P(-1.96 < Z < 1.96) = 0.95$. We have...

$\implies P(-1.96 < U < 1.96) = 0.95 = P(-1.96 < \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} < 1.96) = 0.95$

$\implies P(-1.96(\frac{\sigma}{\sqrt{n}}) < \bar{X} - \mu < 1.96(\frac{\sigma}{\sqrt{n}})) = 0.95$

$\implies P(\bar{X} - 1.96(\frac{\sigma}{\sqrt{n}}) < \mu < \bar{X} + 1.96(\frac{\sigma}{\sqrt{n}})) = 0.95$

This can be generalized to $\bar{X} \pm ME(\bar{X}) = \bar{X} \pm z_{\frac{\alpha}{2}}(\frac{\sigma}{\sqrt{n}})$

($ME$ - margin of error, $SE(\bar{X} = \frac{\sigma}{\sqrt{n}}$ - standard error)

Note this can only be used when the sample comes from independent normal observations.

Below are some common coefficients and their corresponding Z-values
\begin{tabular}{r|r|r}
\hline
  Confidence Coefficient & Confidence Level & Z-Value\\
\hline
0.90 & 90\% & 1.645\\
\hline
0.95 & 95\% & 1.96\\
\hline
0.99 & 99\% & 2.575\\
\hline
\end{tabular}

## CI for the Mean of a Normal Population (Large Sample Size, Variance Unknown)

Same formula as above, except we use sample variance instead of population variance:

$\bar{X} \pm ME(\bar{X}) = \bar{X} \pm z_{\frac{\alpha}{2}}(\frac{s}{\sqrt{n}})$

($ME$ - margin of error, $SE$ - standard error)

(This is due to the large sample size of $n$ allowing us to apply CLT)

## CI for the Mean of a Normal Population (Small Sample Size, Variance Unknown)

Again the same formula as above, except we use the $t$-distribution rather than the Normal.

$\bar{X} \pm ME(\bar{X}) = \bar{X} \pm t^*_{n - 1}(\frac{s}{\sqrt{n}})$

($ME$ - margin of error, $SE$ - standard error)

The value of $t^*_{n - 1}$ is dependent on the confidence level you choose and the degrees of freedom (which is $(n - 1)$)

## Large Sample CI for a Population Proportion

Define the following:

- $\hat{p} = \frac{x}{n}$
- $ME(\hat{p}) = Z^* \cdot SE(\hat{p})$
  - $Z^* =$ Z-Value for specific confidence level
  - $SE(\hat{p}) = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}$

and the final formula for the CI is $\hat{p} \pm ME(\hat{p})$

Conditions that must be met for this:

- Independent observations
- Sampled at random
- $10\%$ condition (sampling over $10\%$ of the true population gives a lack of independence and the $SE$ formula will now *overestimate* the true $SE$)
- Sample size/Success-Failure assumption ($n\hat{p} \geq 10$ and $n(1 - \hat{p}) \geq 10$)

## CI for One Population Variance

Conditions:

- Data comes from a single normal population
- Independent observations
- Variance unknown (hence why we are approximating $\sigma^2$)

We use the fact that $\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n - 1}$

So $P(\chi^2_{(n - 1);1-(\alpha/2)} < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{(n - 1);(\alpha/2)}) \implies P(\frac{(n-1)S^2}{\chi^2_{(n - 1);(\alpha/2)}} < \sigma^2 < \frac{(n-1)S^2}{\chi^2_{(n - 1);1-(\alpha/2)}})$

Which means the CI is $[\frac{(n-1)S^2}{\chi^2_{(n - 1);(\alpha/2)}}, \frac{(n-1)S^2}{\chi^2_{(n - 1);1-(\alpha/2)}}]$

# Module 7 - "Hypothesis Testing"

## One-sided tests

**Step 1: State the null hypothesis ($H_0$) and alternative hypothesis ($H_a$)**

The goal is typically to provide evidence against $H_0$ and conclude $H_a$ by a proof by contradiction. We assume the null is true and hope to provide evidence that contradicts it (which allows us to conclude the alternative)

**Step 2: Check the necessary assumptions**

- Independence
- Randomization
- Less than $10\%$ of the population is sampled
- At least $10$ successes and failures exist ($np \geq 10$, $n(1 - p) \geq 10$) to verify a large enough sample size.

**Step 3: Assume the $H_0$ is true, identify the test statistic, and find its value ($\hat{p}^*$)**

Test statistic = "What distribution is used given the information in the question?"

Normal? $t$? Normal approximation to binomial?

(Note for normal we typically have to standardize to be able to use the Z-table and obtain our quantities)

**Step 4: Find the p-value of the test statistic**

P-Value = $P(X > \hat{p}^*)$ for $p$ being greater in the $H_a$
P-Value = $P(X < \hat{p}^*)$ for $p$ being less than in the $H_a$

where $X$ follows a $Z$-distribution, $t$-distribution, etc. (So use those tables)

**Step 5: State the conclusion**

If the P-Value is $\leq \alpha$-level, we reject $H_0$ and can conclude $H_a$

Conversely, if the P-Value is $> \alpha$-level, we *fail* to reject $H_0$ and *cannot* conclude $H_a$

"There is strong evidence that supports ___"

## Two-sided tests

Nearly exactly the same as one-sided, except for Step 4 where we calculate the P-Value. For two sides, we do...

P-Value = $2 \cdot P(X > \hat{p}^*)$ for $\hat{p}^*$ being positive
P-Value = $2 \cdot P(X < \hat{p}^*)$ for $\hat{p}^*$ being negative

Additionally, make a **directional** conclusion if the alternative uses "$\neq$"

In other words, a positive test statistic value indicates that a mean $\mu$ for example is greater than the hypothesized value, whereas a negative test statistic indicates the mean is less than the hypothesized value.

## Hypothesis testing of one mean

### Variance known, Normal Population (or CLT applies for large sample $n$)

$H_0: \mu = \mu_0$, $H_a: \mu \neq \mu_0$

(This implies a two-sided test)

and $\bar{x} \sim N(\mu, \sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}})$

where the observed test statistic is $Z = \frac{\bar{x} - \mu}{\sigma_{\bar{x}}}$

P-Value = $2 \cdot P(Z > \hat{p}^*)$ for $\hat{p}^*$ being positive
P-Value = $2 \cdot P(Z < \hat{p}^*)$ for $\hat{p}^*$ being negative

### Variance known, Normal Population, $n$ is small

Same as above, no difference


### Variance unknown, $n$ large

Same as above, but use sample standard deviation $s$ instead of $\sigma$

Either the standardized $Z$ or standardized $t$ with $(n - 1)$ degrees of freedom work as the test statistic here.

### Variance unknown, $n$ small

Same as before, just use a $t$-test with $(n - 1)$ degrees of freedom


## Hypothesis testing of one variance

$H_0: \sigma^2 = \sigma_0^2$, $H_a: \sigma^2 \neq \sigma_0^2$ or $\sigma^2 > \sigma_0^2$ or $\sigma^2 < \sigma_0^2$

Assuming the null is true implies we assume $\frac{(n - 1)S^2}{\sigma_0^2} \sim \chi^2_{(n - 1)}$

This means our test statistic is $\chi^2 = \frac{(n - 1)S^2}{\sigma_0^2}$ where the reference distribution is $\chi^2_{(n - 1)}$

Decision rules:

- For $H_a: \sigma^2 \neq \sigma_0^2$
  - We must use the significance level $\alpha$
  - Reject $H_0$ if $\hat{p}^* > \chi^2_{(n - 1);\alpha/2}$ OR if $\hat{p}^* < \chi^2_{(n - 1);1 - \alpha/2}$
- For $H_a: \sigma^2 > \sigma_0^2$
  - Reject $H_0$ if P-value = $P(\chi^2_{(n - 1)} > \hat{p}^*)$ is too small
  - Or reject $H_0$ if $\hat{p}^* > \chi^2_{(n - 1);\alpha}$
- For $H_a: \sigma^2 < \sigma_0^2$
  - Reject $H_0$ if P-value = $P(\chi^2_{(n - 1)} < \hat{p}^*)$ is too small
  - Or reject $H_0$ if $\hat{p}^* < \chi^2_{(n - 1);\alpha}$


# Module 8 - "Errors in Tests, Statistical Power, and Sample Size

## Type I and Type II Errors

Type I Error ($\alpha$): Rejecting $H_0$ when $H_0$ is true.

Type II Error ($\beta$): Failing to reject $H_0$ when $H_0$ is false

## Statistical Power

Power = $\pi$ = $1 - \beta$ = $P($reject $H_0$ | $H_0$ is false$)$ = 1 - $P($fail to reject $H_0$ | $H_0$ is false$)$

The value of power depends on how far the hypothesized value lies from the true value. This distance is called the **effect size**

The effect size can be estimated as the difference between the null value and the observed estimate

A larger effect size implies larger power, and smaller effect sizes are difficult to detect and result in more Type II errors (less power)

Note that power increases as sample size increases.

## Using Power to Determine Sample Size

We will use the example of...

$H_0: p = p_0 = 0.90$, $H_A: p = 0.85$, $\alpha = 0.05$

"At what size $n$ do we achieve Power = $0.80$?"

Write the rule for rejecting $H_0$ in terms of $\hat{p}^*$

$\alpha = P($reject $H_0$ | $H_0$ is true$)$

$0.05 = P(\hat{p} < \hat{p}^*\ |\ p = 0.90$)$

$0.05 = P( \frac{\hat{p} - 0.90}{\sqrt{\frac{0.90(1 - 0.90)}{n}}} < \frac{\hat{p}^* - 0.90}{\sqrt{\frac{0.90(1 - 0.90)}{n}}})$

$0.05 = P(Z < \frac{\hat{p}^* - 0.90}{\sqrt{\frac{0.90(1 - 0.90)}{n}}})$

$\implies -1.645 = \frac{\hat{p}^* - 0.90}{\sqrt{\frac{0.90(1 - 0.90)}{n}}}$

$\implies \hat{p}^* = 0.90 - 1.645 \cdot \sqrt{\frac{0.90(1 - 0.90)}{n}}$

Since power implies that the alternative hypothesis is true, write

Power = $P($reject $H_0$ | $H_0$ is false$)$

$0.80 = P($reject $H_0$ | $p = 0.85)$

$0.80 = P(\frac{\hat{p} - 85}{\sqrt{\frac{0.85(1 - 0.85)}{n}}} < \frac{\hat{p}^* - 0.85}{\sqrt{\frac{0.85(1 - 0.85)}{n}}})$

$\implies P(Z < 0.8416) = 0.80$

$\implies \frac{\hat{p}^* - 0.85}{\sqrt{\frac{0.85(1 - 0.85)}{n}}} = 0.8416$

$\implies \hat{p}^* = 0.85 + 0.8416 \cdot \sqrt{\frac{0.85(1 - 0.85)}{n}}$

Now set these two equations equal to each and solve for $n$:

$0.90 - 1.645 \cdot \sqrt{\frac{0.90(1 - 0.90)}{n}} = 0.85 + 0.8416 \cdot \sqrt{\frac{0.85(1 - 0.85)}{n}}$

$\implies n \approx 253$

## Power formulas

With known standard deviation, significance level, and effect size we can use:

$H_a: \mu > \mu_0 \implies \pi(\mu_a) = P(Z > z_c + \frac{\sqrt{n}(\mu_0 - \mu_a)}{\sigma}$

$H_a: \mu < \mu_0 \implies \pi(\mu_a) = P(Z < -z_c + \frac{\sqrt{n}(\mu_0 - \mu_a)}{\sigma}$

$H_a: \mu \neq \mu_0 \implies \pi(\mu_a) = 1 - P(-z_c + \frac{\sqrt{n}(\mu_0 - \mu_a)}{\sigma} < Z < z_c + \frac{\sqrt{n}(\mu_0 - \mu_a)}{\sigma}$

# Module 9 - "Comparing Groups"

# Module 10 - "Analysis of Categorical Data"

# Module 11 - "Correlation and Simple Linear Regression